{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b89db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# The dimensionality of your embeddings (e.g., 50, 100, 200)\n",
    "EMBEDDING_DIM = 256\n",
    "# The number of nearest neighbors to retrieve in each search step\n",
    "K_NEIGHBORS = 25\n",
    "K_RESULTS = 10\n",
    "\n",
    "def rotate_proj(heads, relations):\n",
    "    \"\"\"\n",
    "    Performs RotatE projection: heads ◦ relations, where ◦ is Hadamard product in the complex plane.\n",
    "    Assumes the first half of the embedding dim is the real part and the second half is imaginary.\n",
    "    \"\"\"\n",
    "    re_head, im_head = torch.chunk(heads, 2, dim=-1)\n",
    "\n",
    "    #Make phases of relations uniformly distributed in [-pi, pi]\n",
    "    pi = 3.14159265358979323846\n",
    "\n",
    "    embedding_range = torch.nn.Parameter(\n",
    "    torch.Tensor([(24.0 + 2.0) / EMBEDDING_DIM]), \n",
    "    requires_grad=False\n",
    "    )\n",
    "\n",
    "    phase_relation = relations/(embedding_range.item()/pi)\n",
    "\n",
    "    re_relation = torch.cos(phase_relation)\n",
    "    im_relation = torch.sin(phase_relation)\n",
    "    \n",
    "    # RotatE complex multiplication:\n",
    "    # re(h*r) = re(h)*re(r) - im(h)*im(r)\n",
    "    # im(h*r) = re(h)*im(r) + im(h)*re(r)\n",
    "    re_projected = re_head * re_relation - im_head * im_relation\n",
    "    im_projected = re_head * im_relation + im_head * re_relation\n",
    "    \n",
    "    projected_vec = torch.cat([re_projected, im_projected], dim=-1)\n",
    "    return projected_vec  # Ensure it has shape (1, dim)\n",
    "\n",
    "def transe_proj(heads, relations):\n",
    "    return heads + relations\n",
    "\n",
    "def find_neighbors_torch(query_vectors, entity_embeddings_torch, k):\n",
    "    \"\"\"\n",
    "    Finds the k-nearest neighbors for one or more query vectors using PyTorch.\n",
    "    This is the GPU-accelerated replacement for the scikit-learn model.\n",
    "\n",
    "    Args:\n",
    "        query_vectors (torch.Tensor): A tensor of shape (num_queries, dim)\n",
    "        k (int): The number of neighbors to find.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_queries, k) containing neighbor indices.\n",
    "    \"\"\"\n",
    "    # Calculate pairwise Euclidean distances between query vectors and all entity embeddings\n",
    "    # `torch.cdist` is highly optimized for this operation on GPU.\n",
    "\n",
    "    # query_vectors = torch.nn.functional.normalize(query_vectors, dim=1)\n",
    "    distances = torch.cdist(query_vectors, entity_embeddings_torch)\n",
    "\n",
    "    # Find the indices of the k smallest distances for each query vector.\n",
    "    # We use `torch.topk` with largest=False to get the smallest values (nearest neighbors).\n",
    "    _, indices = torch.topk(distances, k, dim=1, largest=False)\n",
    "\n",
    "    return indices\n",
    "\n",
    "def project_and_find_neighbors_torch(start_node_id, relation_id, k, entity_embeddings_torch, relation_embeddings_torch):\n",
    "    \"\"\"\n",
    "    Projects a starting node by a relation and finds k-nearest neighbors using PyTorch.\n",
    "    \"\"\"\n",
    "    start_node_vec = entity_embeddings_torch[start_node_id]\n",
    "    relation_vec = relation_embeddings_torch[relation_id]\n",
    "\n",
    "    # Project by adding the relation vector (TransE-style projection)\n",
    "    projected_vec = transe_proj(start_node_vec, relation_vec)\n",
    "\n",
    "    # Project RotatE style\n",
    "    # projected_vec = rotate_proj(start_node_vec, relation_vec)\n",
    "\n",
    "    # Find the k-nearest neighbors to the projected vector.\n",
    "    # We add a dimension to make it a (1, dim) tensor for find_neighbors_torch.\n",
    "    indices = find_neighbors_torch(projected_vec.unsqueeze(0), entity_embeddings_torch, k)\n",
    "\n",
    "    # Return the IDs of the neighboring entities as a set\n",
    "    return set(indices[0].cpu().numpy())\n",
    "\n",
    "def execute_query_torch(query, entity_embeddings_torch, relation_embeddings_torch, entity_to_id, relation_to_id, id_to_entity, id_to_relation, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Executes a multi-part query using PyTorch for all vector operations.\n",
    "    \"\"\"\n",
    "    conjunction_parts = query[\"conjunctions\"]\n",
    "    final_relation = query[\"final_projection\"]\n",
    "\n",
    "    # --- Step 1: Process each part of the conjunction ---\n",
    "    print(\"Step 1: Processing conjunctions...\")\n",
    "    intermediate_results = []\n",
    "    for part in conjunction_parts:\n",
    "        node_name = part[\"node\"]\n",
    "        relation_name = part[\"relation\"]\n",
    "        print(f\"  - Finding neighbors for: {relation_name}({node_name})\")\n",
    "        neighbors = project_and_find_neighbors_torch(node_name, relation_name, K_NEIGHBORS, entity_embeddings_torch, relation_embeddings_torch)\n",
    "        intermediate_results.append(neighbors)\n",
    "        print(f\"    -> Found {len(neighbors)} potential candidates.\")\n",
    "\n",
    "    # --- Step 2: Find the intersection of the results ---\n",
    "    print(\"\\nStep 2: Finding intersection of candidate sets...\")\n",
    "    if not intermediate_results:\n",
    "        print(\"No conjunctions to process. Aborting.\")\n",
    "        return []\n",
    "\n",
    "    intersection_ids = intermediate_results[0].copy()\n",
    "    for other_set in intermediate_results[1:]:\n",
    "        intersection_ids.intersection_update(other_set)\n",
    "\n",
    "    if not intersection_ids:\n",
    "        print(\"Intersection is empty. No results found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  -> Found {len(intersection_ids)} entities in the intersection.\")\n",
    "    intersection_names = [idx for idx in intersection_ids]\n",
    "    print(f\"  -> Intersected nodes: {intersection_names}\")\n",
    "\n",
    "    # --- Step 3: Project the intersection set with the final relation ---\n",
    "    print(f\"\\nStep 3: Projecting intersection with final relation '{final_relation}'...\")\n",
    "    relation_vec = relation_embeddings_torch[final_relation]\n",
    "\n",
    "    # Get the embeddings of all nodes in the intersection\n",
    "    intersection_indices = torch.tensor(list(intersection_ids), device=device, dtype=torch.long)\n",
    "    intersection_embeddings = entity_embeddings_torch[intersection_indices]\n",
    "\n",
    "    # Project all of them by the final relation vector\n",
    "    final_projected_vecs = transe_proj(intersection_embeddings, relation_vec)\n",
    "    # final_projected_vecs  = rotate_proj(intersection_embeddings, relation_vec)\n",
    "\n",
    "    # --- Step 4: Find the nearest neighbors to the final projected vectors ---\n",
    "    print(f\"\\nStep 4: Finding final results (nearest neighbors to final projection)...\")\n",
    "    indices = find_neighbors_torch(final_projected_vecs, entity_embeddings_torch, K_RESULTS)\n",
    "\n",
    "    # Flatten the list of lists and get unique entity IDs\n",
    "    final_result_ids = set(indices.flatten().cpu().numpy())\n",
    "\n",
    "    # get only the intersection\n",
    "    # uniques, counts = indices.unique(return_counts=True)\n",
    "    # final_result_ids = uniques[counts > 1].cpu().numpy()\n",
    "\n",
    "    # Convert IDs back to names for the final result\n",
    "    # final_results = [id_to_entity[idx] for idx in final_result_ids]\n",
    "    final_results = [idx for idx in final_result_ids]\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3a2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(embeddings_folder='embeddings', dicts_folder='dictionaries'):\n",
    "    \"\"\"\n",
    "    Loads embeddings and dictionary files from the specified folders.\n",
    "    The i-th row in an embedding file corresponds to the entity/relation with key 'i' in the dict file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from folders: '{embeddings_folder}' and '{dicts_folder}'...\")\n",
    "\n",
    "    # Load embedding matrices from the embeddings folder\n",
    "    entity_embeddings_np = np.load(os.path.join(embeddings_folder, 'entity_embedding.npy'))\n",
    "    relation_embeddings_np = np.load(os.path.join(embeddings_folder, 'relation_embedding.npy'))\n",
    "\n",
    "    # Load entity dictionary and create a name -> id mapping\n",
    "    entity_to_id = {}\n",
    "    with open(os.path.join(dicts_folder, 'entities.dict'), 'r') as f:\n",
    "        for line in f:\n",
    "            idx, name = line.strip().split('\\t')\n",
    "            entity_to_id[name] = int(idx)\n",
    "\n",
    "    # Load relation dictionary and create a name -> id mapping\n",
    "    relation_to_id = {}\n",
    "    with open(os.path.join(dicts_folder, 'relations.dict'), 'r') as f:\n",
    "        for line in f:\n",
    "            idx, name = line.strip().split('\\t')\n",
    "            relation_to_id[name] = int(idx)\n",
    "\n",
    "    # Create a reverse mapping for easy lookup of names from IDs\n",
    "    id_to_entity = {v: k for k, v in entity_to_id.items()}\n",
    "    id_to_relation = {v: k for k, v in relation_to_id.items()}\n",
    "\n",
    "    print(\"Embeddings and dictionaries loaded successfully.\")\n",
    "    return entity_embeddings_np, relation_embeddings_np, entity_to_id, relation_to_id, id_to_entity, id_to_relation\n",
    "\n",
    "def find_interesting_query_from_triplets(triplets_file, entity_to_id, relation_to_id, k=10):\n",
    "    triplets_by_id = []\n",
    "    indexing_dict = {}\n",
    "\n",
    "    with open(triplets_file, 'r') as f:\n",
    "        for line in f:\n",
    "            h_name, r_name, t_name = line.strip().split('\\t')\n",
    "            if h_name in entity_to_id and r_name in relation_to_id and t_name in entity_to_id:\n",
    "                h_id = entity_to_id[h_name]\n",
    "                r_id = relation_to_id[r_name]\n",
    "                t_id = entity_to_id[t_name]\n",
    "                triplets_by_id.append((h_id, r_id, t_id))\n",
    "\n",
    "                if h_id not in indexing_dict:\n",
    "                    indexing_dict[h_id] = {'in': np.empty((0, 2), dtype=np.int64),\n",
    "                                        'out': np.empty((0, 2), dtype=np.int64),\n",
    "                                        'count': 0}\n",
    "                if t_id not in indexing_dict:\n",
    "                    indexing_dict[t_id] = {'in': np.empty((0, 2), dtype=np.int64),\n",
    "                                        'out': np.empty((0, 2), dtype=np.int64),\n",
    "                                        'count': 0}\n",
    "\n",
    "                indexing_dict[h_id]['out'] = np.vstack([indexing_dict[h_id]['out'], [r_id, t_id]])\n",
    "                indexing_dict[h_id]['count'] += 1\n",
    "                \n",
    "                indexing_dict[t_id]['in']  = np.vstack([indexing_dict[t_id]['in'], [r_id, h_id]])\n",
    "                indexing_dict[t_id]['count'] += 1\n",
    "\n",
    "    tail_target_nodes = [eid for eid, data in indexing_dict.items() if data[\"count\"] > 300]\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(tail_target_nodes)\n",
    "\n",
    "    queries = []\n",
    "    targets = []\n",
    "\n",
    "    for target in tail_target_nodes:\n",
    "        relations =  np.unique(indexing_dict[target]['in'][:,0])\n",
    "        projection_relation = np.random.choice(indexing_dict[target]['out'][:, 0]) if indexing_dict[target]['out'].shape[0] > 0 else None\n",
    "        np.random.shuffle(relations)\n",
    "\n",
    "        if relations.shape[0] < 2 or projection_relation is None:\n",
    "            continue\n",
    "\n",
    "        relation1 = relations[0]\n",
    "        relation2 = relations[1]\n",
    "\n",
    "        head1 = np.random.choice(indexing_dict[target]['in'][indexing_dict[target]['in'][:, 0] == relation1, 1])\n",
    "        head2 = np.random.choice(indexing_dict[target]['in'][indexing_dict[target]['in'][:, 0] == relation2, 1])\n",
    "\n",
    "        target_tails = indexing_dict[target]['out'][indexing_dict[target]['out'][:, 0] == projection_relation, 1]\n",
    "\n",
    "        query_triplets = [\n",
    "            (head1, relation1, -1),\n",
    "            (head2, relation2, -1),\n",
    "            (-1, projection_relation, -1)\n",
    "        ]   \n",
    "\n",
    "        queries.append(query_triplets)\n",
    "        targets.append(target_tails)\n",
    "\n",
    "    return queries, targets, triplets_by_id\n",
    "\n",
    "def parse_triplets_to_query(triplets_by_id):\n",
    "    \"\"\"\n",
    "    Converts a list of triplets into the structured query dictionary.\n",
    "    \"\"\"\n",
    "    if not triplets_by_id:\n",
    "        raise ValueError(\"Triplet array cannot be empty.\")\n",
    "\n",
    "    query = {\"conjunctions\": [], \"final_projection\": None}\n",
    "    final_proj_triplet = triplets_by_id[-1]\n",
    "    query[\"final_projection\"] = final_proj_triplet[1]\n",
    "\n",
    "    for head_id, rel_id, tail_id in triplets_by_id[:-1]:\n",
    "        conjunction = {\"node\": head_id, \"relation\": rel_id}\n",
    "        query[\"conjunctions\"].append(conjunction)\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f84d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from folders: '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/TransE_FB15k_1' and '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/data/FB15k'...\n",
      "Embeddings and dictionaries loaded successfully.\n",
      "\n",
      "Using device: cuda\n",
      "Data moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Define directories for your data\n",
    "# EMBEDDINGS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/RotatE_FB15k_0'\n",
    "EMBEDDINGS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/TransE_FB15k_1'\n",
    "DICTS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/data/FB15k'\n",
    "\n",
    "# Load the data as NumPy arrays first\n",
    "entity_embeddings_np, relation_embeddings_np, entity_to_id, relation_to_id, id_to_entity, id_to_relation = load_data(\n",
    "    embeddings_folder=EMBEDDINGS_DIR,\n",
    "    dicts_folder=DICTS_DIR\n",
    ")\n",
    "\n",
    "# --- 3. Set up PyTorch and move data to GPU if available ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and move them to the selected device\n",
    "entity_embeddings_torch = torch.from_numpy(entity_embeddings_np).to(device)\n",
    "relation_embeddings_torch = torch.from_numpy(relation_embeddings_np).to(device)\n",
    "\n",
    "entity_embeddings_torch = torch.nn.functional.normalize(entity_embeddings_torch, dim=1)\n",
    "relation_embeddings_torch = torch.nn.functional.normalize(relation_embeddings_torch, dim=1)\n",
    "\n",
    "print(\"Data moved to device.\")\n",
    "\n",
    "# --- Automatically find a query from the graph data ---\n",
    "query_triplets_by_id, query_tails, _ = find_interesting_query_from_triplets(\n",
    "    os.path.join(DICTS_DIR, 'merged.txt'), \n",
    "    entity_to_id, \n",
    "    relation_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ece46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Executing Discovered Query ---\n",
      "Parsed Query: {'conjunctions': [{'node': np.int64(5668), 'relation': np.int64(463)}, {'node': np.int64(4203), 'relation': np.int64(1303)}], 'final_projection': np.int64(795)}\n",
      "-------------------------\n",
      "Step 1: Processing conjunctions...\n",
      "  - Finding neighbors for: 463(5668)\n",
      "    -> Found 25 potential candidates.\n",
      "  - Finding neighbors for: 1303(4203)\n",
      "    -> Found 25 potential candidates.\n",
      "\n",
      "Step 2: Finding intersection of candidate sets...\n",
      "Intersection is empty. No results found.\n",
      "\n",
      "--- Query Finished ---\n",
      "Final results: []\n",
      "Found 0 unique entities.\n",
      "-----------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 6\n",
    "\n",
    "query_triplets = query_triplets_by_id[idx] if query_triplets_by_id else None\n",
    "\n",
    "if query_triplets:\n",
    "    # Parse the discovered triplets into the structured query format\n",
    "    example_query = parse_triplets_to_query(query_triplets)\n",
    "\n",
    "    print(\"\\n--- Executing Discovered Query ---\")\n",
    "    print(f\"Parsed Query: {example_query}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    final_results = execute_query_torch(example_query, entity_embeddings_torch, relation_embeddings_torch, entity_to_id, relation_to_id, id_to_entity, id_to_relation)\n",
    "\n",
    "    print(\"\\n--- Query Finished ---\")\n",
    "    print(f\"Final results: {final_results}\")\n",
    "    print(f\"Found {len(final_results)} unique entities.\")\n",
    "else:\n",
    "    print(\"\\nCould not automatically discover a suitable query to run.\")\n",
    "\n",
    "print(\"-----------------------\")\n",
    "\n",
    "[q for q in query_tails[idx] if q in final_results ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdaed22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  269,   311,   426,   958,  1040,  1074,  1242,  1322,  1595,\n",
       "        1840,  1941,  1980,  2025,  2453,  2575,  3005,  3193,  3364,\n",
       "        3805,  3818,  4133,  4502,  4790,  4988,  4998,  5233,  5433,\n",
       "        5477,  5747,  5826,  5924,  6036,  6048,  6122,  6255,  6404,\n",
       "        6514,  6519,  6613,  6779,  6850,  6874,  6932,  7105,  7128,\n",
       "        7240,  7358,  7569,  7758,  7769,  8155,  8611,  8665,  8966,\n",
       "        8990,  9085,  9261,  9482,  9916,  9920,  9942, 10054, 10149,\n",
       "       10156, 10272, 10462, 10567, 10723, 11015, 11314, 11709, 11770,\n",
       "       12262, 12279, 13364, 13673, 13918, 13941, 14076, 14202, 14264,\n",
       "       14296, 14494, 14576, 14771, 14941])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(query_tails[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88843213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from folders: '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/TransE_FB15k_1' and '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/data/FB15k'...\n",
      "Embeddings and dictionaries loaded successfully.\n",
      "\n",
      "Using device: cuda\n",
      "Data moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Define directories for your data\n",
    "# EMBEDDINGS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/RotatE_FB15k_0'\n",
    "EMBEDDINGS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/models/TransE_FB15k_1'\n",
    "DICTS_DIR = '/home/cc/PHD/dglframework/KnowledgeGraphEmbedding/data/FB15k'\n",
    "\n",
    "# Load the data as NumPy arrays first\n",
    "entity_embeddings_np, relation_embeddings_np, entity_to_id, relation_to_id, id_to_entity, id_to_relation = load_data(\n",
    "    embeddings_folder=EMBEDDINGS_DIR,\n",
    "    dicts_folder=DICTS_DIR\n",
    ")\n",
    "\n",
    "# --- 3. Set up PyTorch and move data to GPU if available ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and move them to the selected device\n",
    "entity_embeddings_torch = torch.from_numpy(entity_embeddings_np).to(device)\n",
    "relation_embeddings_torch = torch.from_numpy(relation_embeddings_np).to(device)\n",
    "\n",
    "# entity_embeddings_torch = torch.nn.functional.normalize(entity_embeddings_torch, dim=1)\n",
    "# relation_embeddings_torch = torch.nn.functional.normalize(relation_embeddings_torch, dim=1)\n",
    "\n",
    "print(\"Data moved to device.\")\n",
    "\n",
    "# --- Automatically find a query from the graph data ---\n",
    "query_triplets_by_id, query_tails, triplets = find_interesting_query_from_triplets(\n",
    "    os.path.join(DICTS_DIR, 'merged.txt'), \n",
    "    entity_to_id, \n",
    "    relation_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f98366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implementare ranking al posto di knn per mantenere la stessa valutazione\n",
    "\n",
    "head, rel, tail = triplets[1]\n",
    "\n",
    "# query_vectors = rotate_proj(entity_embeddings_torch[head], relation_embeddings_torch[rel])\n",
    "query_vectors = transe_proj(entity_embeddings_torch[head], relation_embeddings_torch[rel])\n",
    "\n",
    "distances = torch.cdist(query_vectors.unsqueeze(0), entity_embeddings_torch)\n",
    "\n",
    "# Find the indices of the k smallest distances for each query vector.\n",
    "# We use `torch.topk` with largest=False to get the smallest values (nearest neighbors).\n",
    "_, indices = torch.topk(distances, 100, dim=1, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256]) torch.Size([1, 592213, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     0, 294298]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tails = [tail for _,_, tail in triplets]\n",
    "\n",
    "head, rel, true_tail = triplets[11]\n",
    "\n",
    "head = torch.index_select(\n",
    "    entity_embeddings_torch, \n",
    "    dim=0, \n",
    "    index=torch.tensor(head, device=\"cuda\")\n",
    ").unsqueeze(1)\n",
    "\n",
    "relation = torch.index_select(\n",
    "    relation_embeddings_torch,\n",
    "    dim=0,\n",
    "    index=torch.tensor(rel, device=\"cuda\")\n",
    ").unsqueeze(1)\n",
    "\n",
    "tail = torch.index_select(\n",
    "    entity_embeddings_torch, \n",
    "    dim=0, \n",
    "    index=torch.tensor(tails, device=\"cuda\").view(-1)\n",
    ").view(1, len(tails), -1)\n",
    "\n",
    "print(head.shape, relation.shape, tail.shape)\n",
    "\n",
    "scores = (head + relation) - tail\n",
    "\n",
    "scores = torch.norm(scores, p=1, dim=2)\n",
    "\n",
    "argsort = torch.argsort(scores, dim = 1, descending=True)\n",
    "\n",
    "(argsort == true_tail).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba6f725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151606528])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4874b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"tail: {tail}\")\n",
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db3932c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (592213) must match the existing size (256) at non-singleton dimension 1.  Target sizes: [-1, 592213].  Tensor sizes: [256]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tails = [tail \u001b[38;5;28;01mfor\u001b[39;00m _,_, tail \u001b[38;5;129;01min\u001b[39;00m triplets]\n\u001b[32m      3\u001b[39m head, rel, tail = triplets[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mentity_embeddings_torch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtails\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (592213) must match the existing size (256) at non-singleton dimension 1.  Target sizes: [-1, 592213].  Tensor sizes: [256]"
     ]
    }
   ],
   "source": [
    "tails = [tail for _,_, tail in triplets]\n",
    "\n",
    "head, rel, tail = triplets[1]\n",
    "\n",
    "entity_embeddings_torch[head].expand(-1, len(tails)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b63478e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([592213, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_embeddings_torch[head].unsqueeze(0).expand(len(tails),-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
