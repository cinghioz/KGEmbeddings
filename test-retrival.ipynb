{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2240f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "sys.path.insert(0, '/home/cc/phd/KGEmbeddings/codes')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle \n",
    "\n",
    "from codes.model import KGEModel\n",
    "from codes.dataloader import TrainDataset, TestDataset\n",
    "from codes.triplets import TripletsEngine\n",
    "\n",
    "# --- Configuration ---\n",
    "# The dimensionality of your embeddings (e.g., 50, 100, 200)\n",
    "EMBEDDING_DIM = 512\n",
    "MODEL_PATH = \"/home/cc/phd/KGEmbeddings/models/TransE_FB15k_0/\"\n",
    "# MODEL_PATH = \"/home/cc/phd/KGEmbeddings/models/RotatE_FB15k_0/\"\n",
    "DICTS_DIR = '/home/cc/phd/KGEmbeddings/data/FB15k'\n",
    "\n",
    "random.seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "number_of_entities = 14951\n",
    "number_of_relations = 1345\n",
    "\n",
    "args = {\n",
    "    \"model\": \"TransE\",\n",
    "    \"hidden_dim\": EMBEDDING_DIM,\n",
    "    \"gamma\": 24.0,\n",
    "    \"double_entity_embedding\": False,\n",
    "    \"double_relation_embedding\": False,\n",
    "    \"do_train\": False,\n",
    "    \"test_batch_size\": 512,\n",
    "    \"cpu_num\": 16,\n",
    "    \"cuda\": True,\n",
    "    \"test_log_steps\": 1000,\n",
    "    \"nentity\": number_of_entities,\n",
    "    \"nrelation\": number_of_relations,\n",
    "    \"mode\": \"tail-batch\"\n",
    "}\n",
    "\n",
    "class DictToObject:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = DictToObject(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f59818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n"
     ]
    }
   ],
   "source": [
    "kge_model = KGEModel(\n",
    "    model_name=args.model,\n",
    "    nentity=number_of_entities,\n",
    "    nrelation=number_of_relations,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    gamma=args.gamma,\n",
    "    double_entity_embedding=args.double_entity_embedding,\n",
    "    double_relation_embedding=args.double_relation_embedding\n",
    ").to(device)\n",
    "\n",
    "print(\"Loading checkpoint...\")\n",
    "checkpoint = torch.load(os.path.join(MODEL_PATH, 'checkpoint'))\n",
    "init_step = checkpoint['step']\n",
    "kge_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "if args.do_train:\n",
    "    current_learning_rate = checkpoint['current_learning_rate']\n",
    "    warm_up_steps = checkpoint['warm_up_steps']\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "entity_embedding = torch.from_numpy(np.load(os.path.join(MODEL_PATH, 'entity_embedding.npy')))\n",
    "relation_embedding = torch.from_numpy(np.load(os.path.join(MODEL_PATH, 'relation_embedding.npy')))\n",
    "\n",
    "kg = TripletsEngine(os.path.join(DICTS_DIR), from_splits=True)\n",
    "\n",
    "# (head, relation) -> list of tails\n",
    "h2t = defaultdict(list)\n",
    "\n",
    "# (relation, tail) -> list of heads\n",
    "r2h = defaultdict(list)\n",
    "\n",
    "indexing_dict = defaultdict(dict)\n",
    "\n",
    "for triple in kg.triplets:\n",
    "    head, relation, tail = tuple(triple)\n",
    "    h2t[(head, relation)].append(tail)\n",
    "    r2h[(tail, relation)].append(head)\n",
    "\n",
    "    if head not in indexing_dict:\n",
    "        indexing_dict[head] = {'in': np.empty((0, 2), dtype=np.int64),\n",
    "                            'out': np.empty((0, 2), dtype=np.int64),\n",
    "                            'count': 0}\n",
    "    if tail not in indexing_dict:\n",
    "        indexing_dict[tail] = {'in': np.empty((0, 2), dtype=np.int64),\n",
    "                            'out': np.empty((0, 2), dtype=np.int64),\n",
    "                            'count': 0}\n",
    "\n",
    "    indexing_dict[head]['out'] = np.vstack([indexing_dict[head]['out'], [tail, relation]])\n",
    "    indexing_dict[head]['count'] += 1\n",
    "\n",
    "    indexing_dict[tail]['in']  = np.vstack([indexing_dict[tail]['in'], [head, relation]])\n",
    "    indexing_dict[tail]['count'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat con interessante discorso su questo: https://chatgpt.com/c/68c0325c-18c8-832b-b7fe-3eb459d9c9b8\n",
    "# TODO: Implementare predict cont RotatE\n",
    "\n",
    "def predict(head_id, relation_id, tail_id, entity_embeddings, relation_embeddings, mode = \"tail-batch\", top_k=10):\n",
    "    head = entity_embeddings[head_id]\n",
    "    rel = relation_embeddings[relation_id]\n",
    "    tail = entity_embeddings[tail_id]\n",
    "\n",
    "    if mode == \"head-batch\":\n",
    "        target = tail - rel\n",
    "    else:\n",
    "        target = head + rel\n",
    "\n",
    "    # L1 distance to all entities\n",
    "    distances = torch.norm(entity_embeddings - target, p=2, dim=1)\n",
    "\n",
    "    # scores = -distances\n",
    "\n",
    "    # # Softmax normalization\n",
    "    # probs = torch.softmax(scores / 1.0, dim=0)  # (num_entities,)\n",
    "    # best_ids = torch.topk(probs, top_k).indices\n",
    "    # return best_ids, distances[best_ids]\n",
    "\n",
    "    # - to get largest scores\n",
    "    best_ids = torch.topk(-distances, top_k).indices\n",
    "    return best_ids, distances[best_ids]\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def intersection(list_of_lists):\n",
    "    if not list_of_lists:\n",
    "        return set()\n",
    "    result = set(list_of_lists[0])\n",
    "    for lst in list_of_lists[1:]:\n",
    "        result &= set(lst)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1\n",
    "# ids = np.random.randint(0, len(kg.triplets), size=n)\n",
    "# mrr = []\n",
    "# recall = []\n",
    "\n",
    "# # TODO: Fare search in vector space batched\n",
    "\n",
    "# for id in tqdm(ids):\n",
    "#     target_head, target_relation, target_tail = kg.triplets[id]\n",
    "\n",
    "#     # print(\"Target triplet:\", (int(target_head), int(target_relation), int(target_tail)))\n",
    "\n",
    "#     if args.mode == 'head-batch':\n",
    "#         targets = r2h[(target_relation, target_tail)]\n",
    "#     else:\n",
    "#         targets = h2t[(target_head, target_relation)]\n",
    "\n",
    "#     # print(\"All correct answers: \", [int(t) for t in targets])\n",
    "\n",
    "#     metrics = kge_model.single_test_step(kge_model, (target_head, target_relation, target_tail), kg.triplets, args)\n",
    "#     mrr.append(metrics['MRR'])\n",
    "\n",
    "#     # print(\"Target triplet:\", (int(target_head), int(target_relation), int(target_tail)))\n",
    "\n",
    "#     # if args.mode == 'head-batch':\n",
    "#     #     print(\"Target: \", int(target_head))\n",
    "#     # elif args.mode == 'tail-batch':\n",
    "#     #     print(\"Target: \", int(target_tail))\n",
    "\n",
    "#     # print(metrics)\n",
    "\n",
    "#     top_ids, dists = predict(int(target_head), int(target_relation), int(target_tail), entity_embedding, relation_embedding, mode=args.mode, top_k=max(K_NEIGHBORS, int(len(targets)*1.5)))\n",
    "\n",
    "#     # print(torch.isin(top_ids, torch.tensor(targets)))\n",
    "#     recall.append(torch.isin(top_ids, torch.tensor(targets)).sum().item() / len(targets))\n",
    "#     # print(torch.isin(top_ids, torch.tensor(targets)).sum().item() / len(targets))\n",
    "\n",
    "# print(f\"Average MRR over {n} random triplets: {np.mean(mrr)}\")\n",
    "# print(f\"Average Recall over {n} random triplets: {np.mean(recall)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_queries(h2t, indexing_dict, n_queries=100):\n",
    "#     queries = []\n",
    "#     results = []\n",
    "\n",
    "#     for node in tqdm(indexing_dict.keys()):\n",
    "#         if indexing_dict[node]['count'] < 4 or indexing_dict[node]['count'] > 500:\n",
    "#             continue\n",
    "\n",
    "#         if len(queries) >= n_queries:\n",
    "#             break\n",
    "\n",
    "#         elements = indexing_dict[node]['in']\n",
    "#         relations = np.unique(elements[:, 1])\n",
    "#         np.random.seed(266)\n",
    "#         np.random.shuffle(relations)\n",
    "\n",
    "#         query = []\n",
    "#         result = []\n",
    "\n",
    "#         try:\n",
    "#             r1, r2 = relations[:2]\n",
    "#             h1 = elements[elements[:, 1] ==  r1].squeeze()[0]\n",
    "#             h2 = elements[elements[:, 1] ==  r2].squeeze()[0]\n",
    "\n",
    "#             t1 = h2t[(h1, r1)]\n",
    "#             t2 = h2t[(h2, r2)]\n",
    "\n",
    "#             target_tails = set(t1 + t2 + [node])\n",
    "#             target_tails.discard(node)\n",
    "\n",
    "#             query.append([(h1, r1), (h2, r2)])\n",
    "#             result.append(target_tails)\n",
    "\n",
    "#             acc = np.empty((0, 2), dtype=np.int64)\n",
    "#             for tt in target_tails:\n",
    "#                 acc = np.vstack([acc, indexing_dict[tt]['out']])\n",
    "\n",
    "#             h = acc[:, 0]\n",
    "#             r = acc[:, 1]\n",
    "\n",
    "#             # Condition 1: count rows per relation ---\n",
    "#             unique_r, r_counts = np.unique(r, return_counts=True)\n",
    "#             mask1 = r_counts >= 2   # at least 2 edges\n",
    "\n",
    "#             # Condition 2: count distinct h per relation ---\n",
    "#             # drop duplicates by (r,h)\n",
    "#             unique_rh = np.unique(acc, axis=0)\n",
    "#             _, rh_counts = np.unique(unique_rh[:, 1], return_counts=True)\n",
    "#             mask2 = rh_counts >= 2  # at least 2 different h\n",
    "\n",
    "#             # Align arrays\n",
    "#             valid_r = np.intersect1d(unique_r[mask1], np.unique(unique_rh[:, 1])[mask2])\n",
    "\n",
    "#             if len(valid_r) > 0:\n",
    "#                 chosen_r = valid_r[0]   \n",
    "#                 # or np.random.choice(valid_r)\n",
    "#                 filtered_targets = np.unique(acc[r == chosen_r][:, 0])\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#             filtered_targets = set(filtered_targets)\n",
    "#             filtered_targets.discard(node)\n",
    "            \n",
    "#             query.append(chosen_r)\n",
    "#             result.append(set(filtered_targets))\n",
    "\n",
    "#             queries.append(query)\n",
    "#             results.append(result)\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#     return queries, results\n",
    "\n",
    "def find_queries(h2t, indexing_dict, n_queries=100):\n",
    "    queries = []\n",
    "    results = []\n",
    "\n",
    "    for node in tqdm(indexing_dict.keys()):\n",
    "        if indexing_dict[node]['count'] < 4 or indexing_dict[node]['count'] > 500:\n",
    "            continue\n",
    "\n",
    "        if len(queries) >= n_queries:\n",
    "            break\n",
    "\n",
    "        elements = indexing_dict[node]['in']\n",
    "        relations = np.unique(elements[:, 1])\n",
    "        np.random.seed(266)\n",
    "        np.random.shuffle(relations)\n",
    "\n",
    "        query = []\n",
    "        result = []\n",
    "        \n",
    "        pairs = np.array(np.meshgrid(relations, relations)).T.reshape(-1, 2)\n",
    "\n",
    "        # Remove same-element pairs if you only want different values\n",
    "        pairs = pairs[pairs[:,0] != pairs[:,1]]\n",
    "        np.random.shuffle(pairs)\n",
    "\n",
    "        for pair in pairs[:min(5, len(pairs))]:\n",
    "            try:\n",
    "                # r1, r2 = relations[:2]\n",
    "                r1, r2 = pair\n",
    "                h1 = elements[elements[:, 1] ==  r1].squeeze()[0]\n",
    "                h2 = elements[elements[:, 1] ==  r2].squeeze()[0]\n",
    "\n",
    "                t1 = h2t[(h1, r1)]\n",
    "                t2 = h2t[(h2, r2)]\n",
    "\n",
    "                target_tails = set(t1 + t2 + [node])\n",
    "                target_tails.discard(node)\n",
    "\n",
    "                query.append([(h1, r1), (h2, r2)])\n",
    "                result.append(target_tails)\n",
    "\n",
    "                acc = np.empty((0, 2), dtype=np.int64)\n",
    "                for tt in target_tails:\n",
    "                    acc = np.vstack([acc, indexing_dict[tt]['out']])\n",
    "\n",
    "                h = acc[:, 0]\n",
    "                r = acc[:, 1]\n",
    "\n",
    "                # Condition 1: count rows per relation ---\n",
    "                unique_r, r_counts = np.unique(r, return_counts=True)\n",
    "                mask1 = r_counts >= 2   # at least 2 edges\n",
    "\n",
    "                # Condition 2: count distinct h per relation ---\n",
    "                # drop duplicates by (r,h)\n",
    "                unique_rh = np.unique(acc, axis=0)\n",
    "                _, rh_counts = np.unique(unique_rh[:, 1], return_counts=True)\n",
    "                mask2 = rh_counts >= 2  # at least 2 different h\n",
    "\n",
    "                # Align arrays\n",
    "                valid_r = np.intersect1d(unique_r[mask1], np.unique(unique_rh[:, 1])[mask2])\n",
    "\n",
    "                if len(valid_r) > 0:\n",
    "                    chosen_r = valid_r[0]   \n",
    "                    # or np.random.choice(valid_r)\n",
    "                    filtered_targets = np.unique(acc[r == chosen_r][:, 0])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                filtered_targets = set(filtered_targets)\n",
    "                filtered_targets.discard(node)\n",
    "                \n",
    "                query.append(chosen_r)\n",
    "                result.append(set(filtered_targets))\n",
    "\n",
    "                queries.append(query)\n",
    "                results.append(result)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return queries, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddc3a4214014733b195aebe62828659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries, results = find_queries(h2t, indexing_dict, n_queries=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79312823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries, results = find_queries(h2t, indexing_dict, n_queries=10000)\n",
    "\n",
    "# save_dict = {\n",
    "#     'queries': queries,\n",
    "#     'results': results\n",
    "# }\n",
    "\n",
    "# with open('queries-set2.pkl', 'wb') as f:\n",
    "#     pickle.dump(save_dict, f)\n",
    "\n",
    "with open('queries-set2.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "\n",
    "queries = loaded_dict['queries']\n",
    "results = loaded_dict['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3970a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEIGHBORS = 50\n",
    "K_RESULTS = 25\n",
    "\n",
    "metrics = {\n",
    "    \"recall\": [],\n",
    "    \"mrr\": [],\n",
    "    \"hits3\": [],\n",
    "    \"hits5\": [],\n",
    "    \"hits10\": [],\n",
    "}\n",
    "\n",
    "for idx in tqdm(range(1)):\n",
    "    query = queries[idx]\n",
    "    result = results[idx]\n",
    "\n",
    "    head_1, rel_1 = query[0][0]\n",
    "    head_2, rel_2 = query[0][1]\n",
    "\n",
    "    # if args.mode == 'head-batch':\n",
    "    #     targets1 = r2h[(rel_1, head_1)]\n",
    "    #     targets2 = r2h[(rel_2, head_2)]\n",
    "    # else:\n",
    "    #     targets1 = h2t[(head_1, rel_1)]\n",
    "    #     targets2 = h2t[(head_2, rel_2)]\n",
    "\n",
    "    # adapt_retrival = int(((len(targets1) + len(targets2))/2)*1.5)\n",
    "    adapt_retrival = K_NEIGHBORS\n",
    "\n",
    "    ids1, _ = predict(int(head_1), int(rel_1), int(head_1), entity_embedding, relation_embedding,\n",
    "                    mode=args.mode, top_k=max(K_NEIGHBORS, adapt_retrival))\n",
    "\n",
    "    ids2, _ = predict(int(head_2), int(rel_2), int(head_2), entity_embedding, relation_embedding,\n",
    "                    mode=args.mode, top_k=max(K_NEIGHBORS, adapt_retrival))\n",
    "\n",
    "    # print(torch.isin(ids1, torch.tensor(targets1)).sum().item() / len(targets1))\n",
    "    # print(torch.isin(ids2, torch.tensor(targets2)).sum().item() / len(targets2))\n",
    "\n",
    "    # print(targets1)\n",
    "    # print(targets2)\n",
    "\n",
    "    heads_inter = torch.from_numpy(np.intersect1d(ids1.cpu().numpy(), ids2.cpu().numpy()))\n",
    "    # print(torch.isin(heads_inter, torch.tensor(targets)).sum().item() / len(targets))\n",
    "\n",
    "    # print(targets)\n",
    "    # print(heads_inter)\n",
    "\n",
    "    final_targets = list(result[1])\n",
    "    final_rel = int(query[1])\n",
    "    query_finds = []\n",
    "\n",
    "    print(heads_inter)\n",
    "\n",
    "    for h in heads_inter:\n",
    "        ids, dists = predict(int(h), final_rel, int(h), entity_embedding, relation_embedding,\n",
    "                    mode=args.mode, top_k=max(K_RESULTS, int(len(final_targets)*1.5)))\n",
    "        \n",
    "        for t in final_targets:\n",
    "            ranking = (ids == t)\n",
    "            if ranking.sum():\n",
    "                ranking = ranking.nonzero(as_tuple=True)[0]+1\n",
    "                metrics['mrr'].append(1.0 / ranking.item())\n",
    "                metrics['hits3'].append(1.0 if ranking <= 3 else 0.0)\n",
    "                metrics['hits5'].append(1.0 if ranking <= 5 else 0.0)\n",
    "                metrics['hits10'].append(1.0 if ranking <= 10 else 0.0)\n",
    "\n",
    "        query_finds.append(ids.cpu().numpy())\n",
    "\n",
    "    finals = flatten(query_finds)\n",
    "\n",
    "    # print(\"Number of targets:\", len(final_targets))\n",
    "    # print(\"Number of unique candidates:\", len(finals))\n",
    "\n",
    "    # print(\"Final targets:\", final_targets)\n",
    "    number_of_founds = len([ f for f in finals if f in final_targets ])\n",
    "    penalize = [0.0 for _ in range(len(final_targets) - number_of_founds)]\n",
    "\n",
    "    metrics['mrr'].extend(penalize)\n",
    "    metrics['hits3'].extend(penalize)\n",
    "    metrics['hits5'].extend(penalize)\n",
    "    metrics['hits10'].extend(penalize)\n",
    "    metrics['recall'].append(number_of_founds / len(final_targets))\n",
    "\n",
    "print(f\"Average Recall over {len(queries)} complex queries (flat method): {np.mean(metrics['recall'])}\")\n",
    "print(f\"Average MRR over {len(queries)} complex queries (flat method): {np.mean(metrics['mrr'])}\")\n",
    "print(f\"Average Hits@K over {len(queries)} complex queries (flat method): {np.mean(metrics['hits3'])}, {np.mean(metrics['hits5'])}, {np.mean(metrics['hits10'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a079256",
   "metadata": {},
   "source": [
    "Average Recall over 3803 complex queries (flat method): 0.6561470308664061\n",
    "Average MRR over 3803 complex queries (flat method): 0.07793474108706189\n",
    "Average Hits@K over 3803 complex queries (flat method): 0.06498228078953368, 0.102213346991583, 0.1832569004260308"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
